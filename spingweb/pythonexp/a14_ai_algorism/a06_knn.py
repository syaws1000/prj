'''
# K-최근접 이웃(KNN) 알고리즘
1. 도입
    인공지능 세계에서 K-최근접 이웃(KNN)에 대해 알아봅시다.
    친구를 보면 그 사람을 알 수 있라는 말 들어봤을 겁니다. KNN이 바로 그 원리를 이용하는 겁니다.
    쉽게 말해 새로운 친구가 나타났을 때, 그 친구랑 가장 가까운 주변 친구들을 보고 어떤 친구인지 맞히는 방법이랍니다.
    여기서 K는 몇 명의 친구에게 물어볼까? 하는 숫자를 의미합니다.

    과일 바구니로 탐정 놀이를 해봅시다.
    우리 앞에 맛있는 과일들이 섞여있는 바구니가 있다고 상상해 봅시다. 빨갛고 동그란 사과들과, 주황색이고 동그란 오렌지들이 있습니다.
    그런데 갑자기 정체를 알 수 없는 새로운 과일이 하나 툭 떨어졌습니다. 이 과일은 사과일까요? 오렌지일까요?
    우리의 KNN 탐정 규칙을 사용해 봅시다.
    1) 규칙 1 : K를 정해요!!
    먼저 몇 명의 친구에게 물어볼지 정해야 합니다. 가장 가까운 3명에게 물어보자라고 정해봅시다.

    2) 규칙 2:  거리를 재요
    이제 새로운 과일에서 가장 가까운 친구 과일 3개를 찾는 겁니다. 자로 재듯이 거리를 재는 거죠..

    찾아보니, 가장 가까운 친구 3명은 사과 2개와 오렌지 1개였습니다.

    3) 규칙 3 : 다수결로 결정해요
    이제 투표 시간입니다.
        사과 팀 : 2표
        오렌지 팀 : 1표
        
        누가 이겼나요? 바로 사과팀이죠?

    결론  
        새로운 과일은 아마 사과일거야라고 추리하는 겁니다.
        만약, 우리가 처음에 K를 5로 정했다면 가장 가까운 5명을 보고 다수결로 정하면 됩니다.

2. 개요
    K최근접 이웃(K-Nearest Neighbors, KNN)은 새로운 데이터가 주어졌을 때, 기존 데이터 중 가장 가까운 K개의 이웃을 찾아 그 이웃들의 특성을 
    따라 분류하거나 예측하는 간단하고 직관적인 지도 학습 알고리즘입니다.

    새로운 데이터의 정체를 파악하기 위해, 그 주변에서 가장 가까운 K개의 이웃 데이터를 보고 다수결로 결정하는 방식과 같습니다. 예를 들어
    K를 3으로 설정하고 가장 가까운 이웃 3개가 모두 사과라면 새로운 데이터 역시 사과일 것이라고 판단합니다.

    KNN은 특정 모델을 미리 만들어두지 않고 예측 시점에 모든 계산을 수행하므로 게으른 학습(Lazy Learning)이라고도 부릅니다.

3. 핵심 개념
    1) 거리 측정(Distance Metric)
        가깝다는 것을 정의하는 기준으로, 데이터 포인트 간의 거리를 측정해야 합니다. 가장 보편적으로 사용되는 방법은 유클리드 거리입니다.
        두접 사이의 유클리드 거리를 수학식에 의해 계산
    2) K의 선택
        K값은 모델의 성능에 직접적인 영향을 미치는 매우 중요한 파라미터입니다.
        - k가 너무 작을 경우(k=1) : 가장 가까운 데이터 하나에만 의존하므로, 노이즈나 이상치에 매우 민감해집니다. 이로 인해 모델이 훈련
        데이터에만 과하게 최적화된 과대적합(overfitting)이 발생할 수 있습니다.
        - K가 너무 클 경우 : 너무 넓은 범위의 데이터를 참고하게 되어 클래스 간의 경계가 모호해집니다. 이는 모델이 지나치게 단순화하여
        제대로 예측하지 못하는 과소적합으로 이어질 수 있습니다.

        적절한 K값은 데이터의 특성에 따라 다르며, 보통 여러 K값을 테스트하여 가장 성능이 좋은 값을 선택합니다.
        일반적으로는 동점 상황을 피하기 위해 홀수를 K값으로 설정하는 경우가 많습니다.
    3) 차원의 저주(curse of dimemsionality)
        KNN은 데이터의 차원(특성의 갯수)이 늘어날수록 성능이 저하되는 경향이 있습니다. 차원이 커지면 데이터 포인트 간의 거리가 전반적으로 멀어지고
        밀도가 희박해져, 가까운 이웃이라는 개념 자체가 무의미해질 수 있습니다. 따라서 KNN을 사용할 때는 특성 선택이나 차원 축소를 통해 데이터의
        차원을 줄이는 과정이 중요할 수 있습니다.

# 사과와 오렌지를 아삭함,달콤함의 두가지 구별 기준으로 사과일경우과 오렌지인경울 구분 처리..        

'''
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier

# 1. 과일 데이터 만들기(사과와 오렌지)
#    특징(아삭함, 달콤함) 1~7, 이름표(0=사과, 1=오렌지)
#    [아삭함, 달콤함]
fruits_feature = np.array([
    [8,5],[9,4],[8,6],[9,5], # 사과들(아삭함이 높고, 달콤함이 중간)
    [2,8],[1,9],[3,8],[2,7]  # 오렌지들(아삭함이 낮고, 달콤함이 높음)
])
# 이름표: 0은 사과, 1은 오렌지
fruit_labels = np.array([0,0,0,0,1,1,1,1])

# 2. 궁금한 새 과일 등장
#    아삭함이 7이고, 달콤함이 6인 이 과일은 뭘까요?
new_fruit = np.array([[7,6]])

# 3. K-최근접 이웃 탐정 부르기(모델 만들고 훈련하기)
#    가장 가까운 친구 3명(k=3)에게 물어보는 탐정
knn_detective = KNeighborsClassifier(n_neighbors=3)

# 탐정에게 기존 과정 정보를 입력(학습)
knn_detective.fit(fruits_feature, fruit_labels)

# 4.탐정의 추리! (새 과일 예측)
prediction = knn_detective.predict(new_fruit)

# 결과발표
if prediction[0] == 0:
    print("탐정의 추리 결과 : 이 과일은 사과인 것값습니다.")
else:
    print("탐정의 추리 결과 : 이 과일은 오렌지인 것값습니다.")    

# 그림을 확인
plt.figure(figsize=(8,6))
plt.rc('font', family="Malgun Gothic")

# 사과(x표시)와 오렌지(o표시)를 점으로 찍어요
# 사과는 빨간색 x
plt.scatter(fruits_feature[fruit_labels == 0,0], fruits_feature[fruit_labels==0, 1], c="red", marker="x", s=100, label='사과')
# 오렌지는 주황색 동그라미
plt.scatter(fruits_feature[fruit_labels==1,0], fruits_feature[fruit_labels==1, 1], c="orange", marker="o", s=100, label='오렌지')

# 새로운 과일(물음표)은 큰 초록색 별로 표시
plt.scatter(new_fruit[:,0], new_fruit[:,1], c="green", marker="*", s=200, label="새로운 과일")

plt.title("K-최근접 이웃(KNN) 과일 탐정")
plt.xlabel("아삭함 지수")
plt.xlabel("달콤함 지수")
plt.legend()
plt.grid(True)
plt.show()
